{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-20T15:53:30.320974Z","iopub.execute_input":"2022-01-20T15:53:30.321569Z","iopub.status.idle":"2022-01-20T15:53:30.330494Z","shell.execute_reply.started":"2022-01-20T15:53:30.321524Z","shell.execute_reply":"2022-01-20T15:53:30.329669Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam \nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.utils as ku \n\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2022-01-20T16:35:14.935226Z","iopub.execute_input":"2022-01-20T16:35:14.936083Z","iopub.status.idle":"2022-01-20T16:35:14.941831Z","shell.execute_reply.started":"2022-01-20T16:35:14.936045Z","shell.execute_reply":"2022-01-20T16:35:14.941133Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer()\n\n#get the data\ndata = open('../input/mythesis-txt/mythesis.txt').read()\ncorpus = data.lower().replace('\\n','. ').split('. ')\n\ntokenizer.fit_on_texts(corpus)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T15:53:35.255496Z","iopub.execute_input":"2022-01-20T15:53:35.255754Z","iopub.status.idle":"2022-01-20T15:53:35.295636Z","shell.execute_reply.started":"2022-01-20T15:53:35.255720Z","shell.execute_reply":"2022-01-20T15:53:35.295027Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#creating the word index \ntotal_words = len(tokenizer.word_index)+1\ntotal_words","metadata":{"execution":{"iopub.status.busy":"2022-01-20T15:53:35.296641Z","iopub.execute_input":"2022-01-20T15:53:35.296875Z","iopub.status.idle":"2022-01-20T15:53:35.307423Z","shell.execute_reply.started":"2022-01-20T15:53:35.296842Z","shell.execute_reply":"2022-01-20T15:53:35.306726Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#create the input sequences\ninput_sequences= []\nfor line in corpus:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T15:53:35.309782Z","iopub.execute_input":"2022-01-20T15:53:35.310193Z","iopub.status.idle":"2022-01-20T15:53:35.347167Z","shell.execute_reply.started":"2022-01-20T15:53:35.310156Z","shell.execute_reply":"2022-01-20T15:53:35.346534Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#pad the input sequences\nmax_sequences_len = max([len(x) for x in input_sequences])\nprint(max_sequences_len)\n\npadded_input_sequences = np.array(pad_sequences(input_sequences, \n                                       maxlen= max_sequences_len,\n                                       padding='pre'))\n\nprint(padded_input_sequences[:5])","metadata":{"execution":{"iopub.status.busy":"2022-01-20T15:53:35.348323Z","iopub.execute_input":"2022-01-20T15:53:35.348574Z","iopub.status.idle":"2022-01-20T15:53:35.498147Z","shell.execute_reply.started":"2022-01-20T15:53:35.348525Z","shell.execute_reply":"2022-01-20T15:53:35.497311Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#create predictors and label\npredictors, label = padded_input_sequences[:,:-1],padded_input_sequences[:,-1]\nlabel = ku.to_categorical(label, num_classes=total_words)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T15:53:35.499510Z","iopub.execute_input":"2022-01-20T15:53:35.499965Z","iopub.status.idle":"2022-01-20T15:53:35.542876Z","shell.execute_reply.started":"2022-01-20T15:53:35.499912Z","shell.execute_reply":"2022-01-20T15:53:35.542110Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"predictors","metadata":{"execution":{"iopub.status.busy":"2022-01-20T15:53:35.544275Z","iopub.execute_input":"2022-01-20T15:53:35.544546Z","iopub.status.idle":"2022-01-20T15:53:35.550564Z","shell.execute_reply.started":"2022-01-20T15:53:35.544510Z","shell.execute_reply":"2022-01-20T15:53:35.549735Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#the model \nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(total_words, 100, input_length=max_sequences_len-1),\n    tf.keras.layers.Bidirectional(LSTM(200,return_sequences=True)),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Bidirectional(LSTM(64)),\n    tf.keras.layers.Dense(1024,activation='relu',kernel_regularizer=regularizers.l2(0.01)),\n    tf.keras.layers.Dense(total_words, activation = 'softmax')\n])\n\noptimizer ='Adam'\n\nmodel.compile(loss = 'categorical_crossentropy', \n              optimizer = optimizer, \n             metrics = ['acc'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-01-20T16:04:39.463204Z","iopub.execute_input":"2022-01-20T16:04:39.463460Z","iopub.status.idle":"2022-01-20T16:04:40.270892Z","shell.execute_reply.started":"2022-01-20T16:04:39.463432Z","shell.execute_reply":"2022-01-20T16:04:40.270209Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":" history = model.fit(predictors,\n                     label, \n                     epochs=100, \n                     verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T16:04:52.914291Z","iopub.execute_input":"2022-01-20T16:04:52.915131Z","iopub.status.idle":"2022-01-20T16:23:20.022930Z","shell.execute_reply.started":"2022-01-20T16:04:52.915091Z","shell.execute_reply":"2022-01-20T16:23:20.022148Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.DataFrame(history.history)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T16:33:07.073087Z","iopub.execute_input":"2022-01-20T16:33:07.073580Z","iopub.status.idle":"2022-01-20T16:33:07.078068Z","shell.execute_reply.started":"2022-01-20T16:33:07.073539Z","shell.execute_reply":"2022-01-20T16:33:07.077370Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"df.plot(figsize=(8,5))\nplt.xlabel('epochs')\nplt.title/","metadata":{"execution":{"iopub.status.busy":"2022-01-20T16:33:07.728687Z","iopub.execute_input":"2022-01-20T16:33:07.729343Z","iopub.status.idle":"2022-01-20T16:33:07.960821Z","shell.execute_reply.started":"2022-01-20T16:33:07.729304Z","shell.execute_reply":"2022-01-20T16:33:07.960071Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"seed_text = 'Drug discovery for G-protein-coupled receptors'\nnext_words = 100\n\nfor _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list],maxlen = max_sequences_len-1, padding = 'pre')\n        predicted = np.argmax(model.predict(token_list, verbose=0),axis = -1)\n        output_word = \"\"\n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word \n                break\n        seed_text += ' ' + output_word\nprint(seed_text)","metadata":{"execution":{"iopub.status.busy":"2022-01-20T16:45:59.926768Z","iopub.execute_input":"2022-01-20T16:45:59.927208Z","iopub.status.idle":"2022-01-20T16:46:03.704376Z","shell.execute_reply.started":"2022-01-20T16:45:59.927155Z","shell.execute_reply":"2022-01-20T16:46:03.703613Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}